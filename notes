Genome

3/31/17
downloaded funhe genome version 3.0.2 from:
ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCF/000/826/765/GCF_000826765.1_Fundulus_heteroclitus-3.0.2/
downloaded same version gff file from same repository

3/31/17
moved funhe genome and annotation to kodiak folder:
/data/oziolore/fhet/data/genome

unzipped using:
gzip -d reference_funhe.fna.gz > reference_funhe.fna

indexed using:
/data/oziolore/programs/bwa/bwa index reference_funhe.fna


4/3/17
Moved raw reads from stationary computer to cluster using rsync -a (i think, lost actual script)

4/3/17
Mapped reads to heteroclitus genome using script mapping.sh found in
/data/oziolore/fhet/scirpts/mapping.sh
parsed it into 4 scirpts to run faster

4/4/17

fai indexed the fhet genome file using:
/data/oziolore/upload/samtools-1.3/samtools faidx \
/data/oziolore/fhet/data/genome/reference_funhe.fna

Used awk to turn fai to a genome file:
awk -v OFS='\t' {'print $1,$2'} /data/oziolore/fhet/data/genome/reference_funhe.fna.fai >/data/oziolore/fhet/data/genome/reference_funhe_genome.txt

4/5/17

created list of each population using old lists from FG comparison and placed them in:
/data/oziolore/fhet/data/list/

ran coverage stats on populations using:
/data/oziolore/fhet/scripts/cover.sh

4/7/17

Made a sub-genome file by using cat reference_funhe_genome.txt | grep NW_012234471.1 > reference_ahr_genome.txt

Made smaller windowed files just for ahr using 50b10b_winahr.sh (CHANGED SCRIPT TO FULL GENOME) in scripts folder

modified cover.sh and ran just for the scaffold that holds AHR and 50b10bslide
	ran it as cover_ahr_50b.sh
	did not work with this as apparently I have to create bams just for the region that I am interested in if I want to do that
	I will suck it up and just do coverage on that tiny window for all
	for that I will have to expand the 50b10b.bed to the full genome
ran this using cover_50b.sh

estimated allele frequencies of ahr deletion:
	used median coverage
	running script in R and estimating coverage of bb/(gb+sp/2) over the first 600000 bases of the scaffold of ahr
	then estimated over the deletion region 640000:700000 roughly
	divided coverage of bb in deletion region over average to see % individuals with the ahr allele in  resistant BB pop
	calculated allele frequency and propagated error. all found in coverage.r in ~/share/fhe/coverage

made subfile of only the AHR carrying scaffold using:
	cat $pop.50b10b.qf.cov | grep NW_012234474.1 > $pop.50b10b.subashr.cov

6/30/17

Installed samtools version 1.5 in upload folder
used:
./configure --prefix=/where/to/install
make
make prefix=/home/oziolore install

will try iterating over bam files with samtools to index them using

ls /home/oziolore/restoreFromData/fhet/data/align/*.bam | xargs -n1 -P5 samtools index
^ can be found in scripts/indexing.sh

installed htslib in the same manner as samtools


-----

moved installation to all programs (bamtools, samtools, htslib, freebayes) to location /home/oziolore/restoreFromData/program/

added all to paths

---------------------


merging all bam files - used a script to create a list of all bams and made it into merged_bams.list in the fhet/data/align folder
used bamtools merge -list -out to merge all bams. Created a document 86G

7/3/17

used samtools depth (script coverage_allbases.sh) to get per base coverage across the genome
using awk and bedtools to throw out sites above the arbitrarily determined one and convert the coverage file into a bed formatted file - find script as hicov_tobed.sh

used sort -R to get a random subsamlple of 10Mb so that I can assess threshold of high coverage regions

7/4/17

using R to do assess high coverage threshold I chose 300 as high coverage, which removes an estimated 3% of the genome
will verify this after the high coverage script is out.

used hicov.sh to select only high coverage regions (merging them with bedtools) - estimated ~ 27Mb thrown out for high coverage

used freebayes to call variants (freebayes_calls.sh) and put the vcf in the folder varcall in data

7-5-17

reinstalled htslib (ver 1.5) and installed angsd (0.919) along with it

created a keepsites.bed file in the angsd folder using a script reverse to hicov.bed (find in /data/angsd)

created a 50Mb keep file for angsd to do SAF estimation on it (find in /data/angsd)

Changed both keep files to a 1-indexed format and indexed them with angsd sites -index <file>

ran angsd on all populations with -doSaf 1 for SFS estimation (restricting to 50Mb region and 10 min ind for each population)

7-6-17

realized I accidentally estimated unfolded allele frequency spectrum - moved all files to unfolded subdirectory (scripts and data/angsd)

now re-running scripts to calculate .saf and .sfs on a folded spectrum using fhet genome as -anc

ran all .saf and .sfs, but when I try to run -doThetas I get segmentation fault
	i tried re-indexing both the keepsites.file and the genome to fix this
7-7-17

re-installed angsd and htslib to see if that helps.

re-downloaded the genome (just for the purposes of angsd) into a separate folder (genome2) and prepared it (bwa index and samtools faidx)

broke up the keep sites segments into 11 files (roughly 50000 lines long) and will break up angsd commands for each population into 11 chunks and try to merge them after!

7-12-17

variant calling finished - I started the filtration proces using snp_filter.sh

started indexing second set of mapped reads to try thetas on them (started with a try on BB)

looking at the attempt where I segmented the genome, only two of the segments worked (6 and 7), but everything else gave me a segmentation fault at some point

7-13-17

I sorted my keep file using sort -k1 keepsites.file > keepsites_sorted.file

I indexed that keepsites_sorted.file

I made a chrs.txt file (to restrict angsd to only those sites) using cut -f1 keepsites_sorted.file | sort | uniq > chrs.txt

Re-ran SJ -dothetas code with new keepsites and -rf option for chrs.txt

7-24-17

Computer scientist at Baylor found a way to make the program run-able. Trying to re-run the script with the modified version of angsd today.

7-27-17

Re-did the snp-filter script to output to stdout (had missed that part and I was wondering why it wouldn't stdout for me...duh)
looking into using vcftools to call pi (don't know if it does theta yet, but will look into it).
