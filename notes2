9-14-17

decided to just go ahead and map all the reads to the chromosome ordered genome
downloaded genome from Jeff today. Moved it to the kodiak cluster
mapping everything to that with mapping.sh placed in scripts2/
	split into 8 scripts for speed

indexing genome with bwa index $genome
creating fai with samtools fai $genome

9-18-17

removed empty reads with
find /directory/to/reads/ -size 0 -print0 | xargs -0 rm
find /directory/to/reads/ -size -93 -print0 | xargs -0 rm

re-mapped 50-101 due to typo in script

found out that program for fst callign was crashing due to 2 arguments being part of the GL call, but it needs 3
re-done vcf has three arguments for GL, so as soon as mapping is done, I will re-call vcf on chromosomal alignment and run from there!

copied scripts bammerge,coverage,hicov.sh to scripts2

I will execute them in order to get hicov regions to exclude before calling variants.

9-19-17

used the following line to create a genome file in the /genome2/ subfolder
awk -v OFS='\t' {'print $1,$2'} $fai > $genome

started using coverage.sh (scripts2) folder to get per base samtools derived coverage to get windows to exclude from calling vcf

9-20-17

ran hicov_bed.sh script to exclude regions of high coverage (above 300).
	4th column is now how many bases are in the excluded region (can be used to see how much of the genome was excluded).

started calling variants on the chromosome ordered genome.

9-21-17

I am trying to run freebayes in parallel in order to get a better run time. Not in parallel it should take 10 days....

SNeP seems to be an interesting tool to figure out effective population sizes (use it after vcf is called)

9-22-17

realized i didn't index the merged bam file...which is why this could potentially be taking this long...
samtools index on .bam file - trying out parallel again.
Yep, parallel works...ran freebayes-parallel_chr[1-4].sh broke it down to scaffold length chunks and ran it that way. RIDDICK

created keep files using the old keep scripts (indexed them with angsd)

re-did all the lists and re-ran freebayes-parallel with new list of pops (i don't think it really matters, but I did it just in case)
	old lists had the old bam destinations, but those are only used for sample name and not for actual destination.

9-23-17

re-indexed the .file for 50Mb...seemed that the index was older than the file....no clue why!

used this to prepare a list of files from scaffolds to be merged:

find *.vcf* > ../chr_list.txt

cat chr_list.txt | sort -V | grep "^chr" > files
cat chr_list.txt | sort | grep -v "^chr" >> files

9-26-17

installed vcftools (to use vcf-concat to merge files)

used cat chr_list.txt | awk '{print "destination"$1}p' > chr_dest
on file containing sorted list of chrs vcf files

using vcf-concat as:
	vcf-concat -c -f chr_list_destination.txt > outfile

9-27-17

removed old vcf files and the _chr.vcf. Only left merged vcf and new call vcf
	calling on chromosomes failed midway when they were not separated

first entry from new directory.

-reindexed keepsites2.file
	used angsd sites index keepsites2.file
-reindexed fasta with samtools faidx file (i think in the copy everything got converted to the same time so it messed up estimates of when things were indexed)

ran thetas on all pops

installed admixture and plink

snps filtered and output is filtered_chr.vcf.gz

bgzipped the filtered_chr after zcatting it
tabixed the bgzipped one

started the fst calculations

9-28-17

Fst finished running. Converted all fst and neut statistics into 5kb files. About to download and run on personal computer.

Running Admixture for k1-7 with modified bim and fam files (fam has genders now)

tried running SNeP

9-29-17

SNeP just never finished ld estimates.
I did estimates with Plink and they were instantaneous. Reworking the ld file to match what SNeP needs

10-12-17

Did coverage over all regions to get at deletion questions.

did plink on full genome

started fst run on m vs. f

10-20-17

starting to run fastNGSadmix. Using BBGB and following the ADMIXTURE reference panel directions.

use this when generating sites for angsd
	 cat refPanel_BBGB.2.P.txt | awk '{print $2,$3,$5,$6}' | sed "s/^/chr/" | tail -n +2  > refPanel_BBGB.sites

found that I was using a wrong pop list, so i rerun everything with 100 bootstraps and a new set
thus I now have probabilities for each individual that are a results of 100 bootstraps. exporting to view in R

after getting all probabilities I used this line to get the order in accordance, while adding >> for all other pops - GB, SP, BNP, SJ, PB, VB, BB
	cat file | grep -n "GB" | cut -f1 -d':' | awk '{s=$1-1}{print $1}' > order.txt


10-21-17

realized i had run the admixture only on GB and all the resistants. Running everything over with both GB and SP and all resistants...............

11-1-17

subsetting vcf into chromosome 1 alone (scirpts2/vcf_subset/)

getting coverage for the region 700-900kb into chr1 (scripts2/coverage/)

11-8-17

Using plink -blocks to estimate haplotype blocks per population - need to make .vcf per population first

11-9-17

plink blocks doesn't do much. I am working on r2 in all pops within 1Mb window to see if there are any blocks and how big they are.

12-12-17

using tabix and bgzip to get vcf files that only contain shared/resistant/intermediate outliers.
find script in scripts2/haplo/

12-13-17

used r script to only choose 25 mb of the genome at random
converted those regions to tabix formated chr:start-end with awk
feeding into tabix to create vcf of those sites
find scripts in scripts2/phylo

12-19-17

removed chromosome1 from zshared outlier analysis to see if that's dominating the signal.

3-8-18
Uploading to NCBI using Aspera command line options:

ascp -i <path/to/key_file> -QT -l100m -k1 -d <path/to/folder/containing files> subasp@upload.ncbi.nlm.nih.gov:uploads/oziolor@gmail.com_d4N6nwjy

4-26-18

Downloaded Noah's lift data - genomic positions in windows2

sorted with sort -k1,1 and converted to bed (with tabs in awk)
cat noah.1kb.bed | sort -n -k1.4 > new_noah.1kb.bed

4-27-18

subsampling all populations for fst calling to see if W&C estimator is making issues because of correction for sample size

augmented the fst calling script to only select 24 samples at random from each population and then sort them to make target and background for wcfst and pfst - find in scripts2/fst/fst_pairwise_sub.sh

5-1-18

used a line to select 24 random samples and use those to call fst

grep -n GB $popfile | cut -f 1 -d ":" | awk '{s=$1-1}{print s}' | tr '\n' ',' | sed 's/,$//' | tr ',' '\n' | shuf -n 24 | sort | tr '\n' ',' | sed 's/,$//' > /data/oziolore/fhet/data/list2/GB_sub.txt
